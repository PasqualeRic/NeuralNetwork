{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10190077,"sourceType":"datasetVersion","datasetId":6295778},{"sourceId":10190185,"sourceType":"datasetVersion","datasetId":6295862},{"sourceId":198058,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":168920,"modelId":191272},{"sourceId":198059,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":168921,"modelId":191273},{"sourceId":198074,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":168931,"modelId":191283},{"sourceId":198170,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":169018,"modelId":191369}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contrastive Representation Learning for Electroencephalogram Classification\n\nAuthor: Pasquale Ricciulli (2115446)","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport glob\nfrom scipy.signal import butter, filtfilt\nfrom scipy.signal import resample_poly\nimport librosa\nimport pickle\nfrom collections import Counter\nimport warnings\nfrom scipy.io import loadmat\nimport random\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The code reduces the size of a preprocessed EEG dataset in .mat (MATLAB) format by applying two main strategies:\n___\n\n* **Trial Reduction:** \n\n    * A random subset of trials is selected (one-third of the total trials per file).\n\n* **Data Reduction:**\n\n    * Number of Channels: Reduced to one-third.\n    * Number of Samples: Reduced to half.\nThe resulting dataset is saved in .npz format (compatible with NumPy).","metadata":{}},{"cell_type":"code","source":"#riduzione dataset seed\n\ninput_path = '/kaggle/input/seed-dataset/Preprocessed_EEG'\noutput_path = '/kaggle/working/reduced_dataset'\n\nos.makedirs(output_path, exist_ok=True)\n\n# Parametri per la riduzione\nchannel_reduction_ratio = 3  # Dividi il numero di canali per 3\nsample_reduction_ratio = 2   # Dividi il numero di campioni per 2\n\nmat_files = [f for f in os.listdir(input_path) if f.endswith('.mat') and f != 'label.mat']\nprint(f\"Trovati {len(mat_files)} file .mat nella directory.\")\n\nfor file_name in mat_files:\n    file_path = os.path.join(input_path, file_name)\n    print(f\"Processando il file: {file_name}\")\n    \n    mat_data = loadmat(file_path)\n    \n    # Trova tutte le chiavi che rappresentano trial (chiavi con strutture comuni come djc_eeg, ys_eeg, ecc.)\n    trial_keys = [key for key in sorted(mat_data.keys()) if not key.startswith('__') and isinstance(mat_data[key], np.ndarray)]\n    print(f\"Trial trovati in {file_name}: {trial_keys}\")\n    \n    if not trial_keys:\n        print(f\"Nessun trial trovato in {file_name}, salto il file.\")\n        continue\n    \n    # Seleziona casualmente un terzo dei trial\n    random.seed(42) \n    selected_keys = random.sample(trial_keys, len(trial_keys) // 3)\n    \n    for key in selected_keys:  # Itera SOLO sui trial selezionati\n        data = mat_data[key]\n        \n        # Riduzione del numero di canali\n        num_channels = data.shape[0]\n        reduced_channels = num_channels // channel_reduction_ratio\n        reduced_data = data[:reduced_channels, :]\n        \n        # Riduzione del numero di campioni\n        num_samples = reduced_data.shape[1]\n        reduced_samples = num_samples // sample_reduction_ratio\n        reduced_data = reduced_data[:, :reduced_samples]\n        \n        output_file = os.path.join(output_path, f\"{file_name.replace('.mat', '')}_{key}.npz\")\n        np.savez(output_file, data=reduced_data)\n\nprint(f\"Dataset ridotto salvato in: {output_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code preprocesses EEG signals into `1-second epochs` for further analysis or modeling.\n___\n\n**1. Input and Output**\n\n**Input:** `.npz` files containing multichannel EEG signals stored in the `input_folder`.\n\n**Output:** Preprocessed `.npz` files stored in the `output_folder`, with:\n\n* Data divided into 1-second epochs.\n* Labels assigned to each file based on a predefined list.\n  \n**2. Preprocessing Steps**\n\n**File Loading:**\n\nReads `.npz` files and iterates over each key in the dataset.\nExtracts data with shape `(n_channels, n_samples)`.\n\n**Epoch Division:**\n\n**Splits the signal into 1-second epochs:**\n\n`epoch_length = fs = 200` (200 samples per second for a 200 Hz sampling rate).\nComputes the total number of complete epochs: `n_epochs = n_samples // epoch_length`.\nEach epoch has shape (n_channels, epoch_length).\n\n**Signal Cleaning:**\n\nRemoves outliers by setting values above `upper_limit = 500 µV` to 0.\n\n**Label Assignment:**\n\nUses a predefined list of labels (`labels`) to assign a label to each file.\nLabel is determined based on the file's index and the modulo of the label list length.\n\n**4. Outputs**\nFor each input file, the output contains:\n\nPreprocessed data:\nShape: `(n_epochs, n_channels, 200)`.\nAssigned label:\nInteger representing the file’s category based on the `labels` list.","metadata":{}},{"cell_type":"code","source":"#Preprocessamente per epoche da 1 secondo\ninput_folder = '/kaggle/working/reduced_dataset'\noutput_folder = '/kaggle/working/preprocessed_1_sec'\nos.makedirs(output_folder, exist_ok=True)\n\n# Ignora i warning\nwarnings.filterwarnings(\"ignore\")\n\n# Parametri per la segmentazione e il filtro\nfs = 200  # Frequenza di campionamento in Hz\nepoch_length = fs  # Campioni in 1 secondo (200 campioni per 200 Hz)\nupper_limit = 500  # Valore massimo per la pulizia del segnale\n\n# Etichette definite\nlabels = [1, 0, -1, -1, 0, 1, -1, 0, 1, 1, 0, -1, 0, 1, -1]  # 15 etichette\n\n# Funzione per il filtro band-pass Butterworth\ndef butter_bandpass(lowcut, highcut, fs, order=5):\n    nyquist = 0.5 * fs\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef plot_signals(original_signal, cleaned_signal, channel_idx, file_name):\n    plt.figure(figsize=(15, 10))\n\n    plt.subplot(2, 1, 1)\n    plt.plot(original_signal, label='Originale', color='blue')\n    plt.title(f\"{file_name} - Canale {channel_idx} (Originale)\")\n    plt.xlabel(\"Campioni\")\n    plt.ylabel(\"Ampiezza\")\n    plt.legend()\n\n    plt.subplot(2, 1, 2)\n    plt.plot(cleaned_signal, label='Pulito (±500 µV)', color='red')\n    plt.title(f\"{file_name} - Canale {channel_idx} (Pulito)\")\n    plt.xlabel(\"Campioni\")\n    plt.ylabel(\"Ampiezza\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\nfile_paths = glob.glob(f\"{input_folder}/*.npz\")  # File di input\n\nfor file_path in file_paths:\n    try:\n        # Carica il file .npz\n        file_name = os.path.basename(file_path).replace('.npz', '')\n        print(f\"Processando il file: {file_name}\")\n\n        npz_data = np.load(file_path)\n        \n        for key in npz_data.keys():\n            data = npz_data[key]  # Forma: (n_channels, n_samples)\n            print(f\"Elaborazione della chiave: {key} - Forma: {data.shape}\")\n            \n            # Epoche di 1 secondo\n            n_channels, n_samples = data.shape\n            n_epochs = n_samples // epoch_length  # Numero di epoche complete\n            epochs = []\n\n            for i in range(n_epochs):\n                start_idx = i * epoch_length\n                end_idx = start_idx + epoch_length\n                epoch = data[:, start_idx:end_idx]  # Estrai epoca\n\n                # Pulizia del segnale (valori superiori a ±500 µV)\n                epoch[epoch > upper_limit] = 0\n\n                epochs.append(epoch)\n\n            epochs = np.array(epochs)  # Forma: (n_epochs, n_channels, epoch_length)\n            print(f\"Dati suddivisi in epoche per la chiave {key}: {epochs.shape}\")\n\n            # Estrazione del numero per la label\n            file_number = int(file_name.split('_')[0])  # Prendi il numero del file\n            print(file_number)\n            label_index = (file_number - 1) % len(labels)\n            label = labels[label_index]\n\n            output_path = os.path.join(output_folder, f\"processed_{file_name}_{key}.npz\")\n            np.savez(output_path, data=epochs, label=label)\n            print(f\"File salvato con etichetta {label} in: {output_path}\")\n\n    except Exception as e:\n        print(f\"Errore durante il preprocessing del file {file_path}: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code performs a **split of preprocessed EEG data into training and testing sets**, moving the respective files into separate folders for further use.\n\n**1. Define Input and Output Directories**\n\n**Input Folder:** Preprocessed `.npz` files from the pipeline.\n\n**Output Folders:**\n* `train_folder`: Destination for training set files.\n* `test_folder`: Destination for testing set files.\n  \nEnsures both folders exist by creating them if necessary using `os.makedirs`.\n\n**2. Load Preprocessed Files**\n\nGathers all `.npz` files from the input folder:\n```\nfile_paths = glob.glob(f\"{input_folder}/*.npz\")\n```\nPrints the total number of files found.\n\n**3. Split Dataset**\n\nSplits the dataset into **80% training** and **20% testing** using `train_test_split`:\n```\ntrain_files, test_files = train_test_split(file_paths, test_size=0.2, random_state=42)\n```\n\nRandomization is controlled by `random_state=42` for reproducibility.\n\n**4. Move Files to Train and Test Folders**\n\nA helper function `save_files`:\nIterates through a list of file paths.\nMoves each file to the corresponding folder (`train_folder` or `test_folder`).\nUses `os.rename` to transfer files and prints the operation for logging.\n\n**5. Output**\n\nAfter running, the files are organized as:\n```\n/kaggle/working/train_set/ -> Contains 80% of the preprocessed files.\n/kaggle/working/test_set/  -> Contains 20% of the preprocessed files.\n```\nPrints confirmation of the split and the number of files in each set.\n","metadata":{}},{"cell_type":"code","source":"#Split del dataset per train e test\n\ninput_folder = '/kaggle/working/preprocessed_1_sec'\ntrain_folder = '/kaggle/working/train_set'\ntest_folder = '/kaggle/working/test_set'\nos.makedirs(train_folder, exist_ok=True)\nos.makedirs(test_folder, exist_ok=True)\n\n# Ignora i warning\nwarnings.filterwarnings(\"ignore\")\n\nfile_paths = glob.glob(f\"{input_folder}/*.npz\")\nprint(f\"Trovati {len(file_paths)} file nel dataset elaborato.\")\n\n# Suddivisione in train e test\ntrain_files, test_files = train_test_split(file_paths, test_size=0.2, random_state=42)\nprint(f\"File di train: {len(train_files)}, File di test: {len(test_files)}\")\n\n# Funzione per spostare i file nelle rispettive cartelle\ndef save_files(file_list, destination_folder):\n    for file_path in file_list:\n        file_name = os.path.basename(file_path)\n        destination_path = os.path.join(destination_folder, file_name)\n        os.rename(file_path, destination_path)\n        print(f\"File spostato: {file_name} -> {destination_folder}\")\n\nsave_files(train_files, train_folder)\nsave_files(test_files, test_folder)\n\nprint(\"Divisione in train e test completata.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PREPROCESSING AND REORDERING**","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# ------------------- Funzioni per Salvare e Caricare Modelli -------------------\nsave_path = \"/kaggle/working/models\"\nos.makedirs(save_path, exist_ok=True)\n\ndef save_model(model, path):\n    torch.save(model.state_dict(), path)\n    print(f\"Modello salvato in {path}\")\n\n\n#tolgo il module \ndef load_model(model, path, device=\"cuda\"):\n    state_dict = torch.load(path, map_location=device)\n    if \"module.\" in list(state_dict.keys())[0]:\n        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n    model.load_state_dict(state_dict)\n    print(f\"Modello caricato da {path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"********","metadata":{}},{"cell_type":"markdown","source":"This code performs data augmentation and chunking for EEG data, and includes functionality for extracting labels based on file names.\n___\n\n#### Augmentation Functions\n\nThese functions apply transformations to augment the EEG data, increasing variability and robustness for downstream tasks.\n\n* min_max_amplitude_scale(data, scale_min=0.5, scale_max=2):\n  Scales the amplitude of the signal by a random factor between 0.5 and 2.\n* time_shift(data, shift_min=-50, shift_max=50):\n  Shifts the signal in time by a random number of samples within the range [-50, 50].\n* dc_shift(data, shift_min=-10, shift_max=10):\n  Adds a constant (DC shift) to the signal, chosen randomly between -10 and 10.\n* zero_masking(data, mask_min=0, mask_max=150):\n  Masks a random segment of the signal (sets values to 0) with a random length between 0 and 150 samples.\n* add_gaussian_noise(data, sigma_min=0, sigma_max=0.2):\n  Adds Gaussian noise to the signal, with a standard deviation chosen randomly between 0 and 0.2.","metadata":{}},{"cell_type":"code","source":"#AUGMENTATION E CHUNK DATI\ndef min_max_amplitude_scale(data, scale_min=0.5, scale_max=2):\n    scale_factor = random.uniform(scale_min, scale_max)\n    return data * scale_factor\n\ndef time_shift(data, shift_min=-50, shift_max=50):\n    shift_samples = random.randint(shift_min, shift_max)\n    return np.roll(data, shift_samples)\n\ndef dc_shift(data, shift_min=-10, shift_max=10):\n    shift_value = random.uniform(shift_min, shift_max)\n    return data + shift_value\n\ndef zero_masking(data, mask_min=0, mask_max=150):\n    mask_size = random.randint(mask_min, mask_max)\n    start_idx = random.randint(0, len(data) - mask_size)\n    data[start_idx:start_idx+mask_size] = 0\n    return data\n\ndef add_gaussian_noise(data, sigma_min=0, sigma_max=0.2):\n    sigma = random.uniform(sigma_min, sigma_max)\n    noise = np.random.normal(0, sigma, len(data))\n    return data + noise\n\ndef apply_random_transformations_class(channel_data):\n    transformations = [min_max_amplitude_scale, time_shift, dc_shift, zero_masking, add_gaussian_noise]\n    selected_transform = random.choice(transformations)\n    transformed_data = selected_transform(channel_data.copy())\n    return transformed_data\n    \ndef apply_random_transformations(channel_data):\n    transformations = [min_max_amplitude_scale, time_shift, dc_shift, zero_masking, add_gaussian_noise]\n    selected_transforms = random.sample(transformations, 2)\n    transformed_data_1 = selected_transforms[0](channel_data.copy())\n    transformed_data_2 = selected_transforms[1](channel_data.copy())\n    return transformed_data_1, transformed_data_2\n    \n    \ndef chunk_data(data, chunk_size=4000):\n    \"\"\"Divide i dati in chunk della dimensione specificata.\"\"\"\n    chunks = []\n    num_chunks = len(data) // chunk_size\n\n    for i in range(num_chunks):\n        start_idx = i * chunk_size\n        end_idx = (i + 1) * chunk_size\n        chunks.append(data[start_idx:end_idx])\n    \n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code implements a **Convolutional Encoder** designed to process sequential data, such as EEG signals.\n___\n### 1. Constructor (__init__)\n\nThe constructor defines the layers and operations of the network. The network consists of three main parts: parallel convolutional paths, repeated blocks, and a final block.\n\n#### Parallel Convolutional Paths\nThree convolutional paths operate in parallel, each capturing features at different temporal scales:\n\nConv1D with kernel size 128:\n\nAdds reflective padding to preserve the original sequence length.\nApplies 100 convolutional filters to extract features over a large temporal window.\n\nConv1D with kernel size 64:\n\nSimilar to the first, but with a smaller kernel size, capturing mid-range temporal dependencies.\n\nConv1D with kernel size 16:\n\nUses a smaller kernel size to capture fine-grained temporal features.\nThese parallel paths allow the network to process the signal at multiple scales simultaneously.\n\n#### Dense Layer for Merging\n\nAfter the parallel paths, their outputs are concatenated and combined through a fully connected (Linear) layer:\n\nMerges the 250 features (100+100+50) into a unified representation.\n#### Repeated Blocks\n\nThe encoder includes 4 repeated blocks for deeper processing of the signal. Each block consists of:\n\nA ReLU activation for non-linearity.\nBatch normalization to stabilize training.\nA Conv1D layer with kernel size 64 and 250 filters.\nResidual connections are employed: each block adds its input to the output, allowing for efficient gradient flow and preventing vanishing gradients.\n\n#### Final Block\n\nThe final layer reduces the dimensionality of the output:\n\nApplies another convolution with a kernel size of 64 and output_dim filters.\nReLU activation and batch normalization ensure stability and non-linearity.\n___\n### 2. forward: Data Flow\nThe forward method defines how the data flows through the network.\n\n#### Input\n\nThe input is a batch of sequences with the shape:\n`\n[batch_size, sequence_length, channels]\n`\n\nFor example, for EEG data with 1 channel, 200 samples per sequence, and a batch size of 32:\n`\n[32, 200, 1]\n`\n#### Transpose Input\n\nThe input is transposed to match PyTorch's Conv1d requirements:\n\n`x = x.permute(0, 2, 1)`\n\nNew shape:\n\n`[batch_size, channels, sequence_length]`\n\n### Parallel Convolutional Paths\n\nThe data passes through the three parallel convolutional paths:\n```\nx1 = self.conv1d_128(x)  # [batch_size, 100, sequence_length]\nx2 = self.conv1d_64(x)   # [batch_size, 100, sequence_length]\nx3 = self.conv1d_16(x)   # [batch_size, 50, sequence_length]\n```\n#### Concatenate Outputs\n\nThe outputs of the three paths are concatenated along the channel axis:\n\n`x_cat = torch.cat([x1, x2, x3], dim=1)  # [batch_size, 250, sequence_length]`\n\n#### Dense Layer\n\nThe concatenated features are passed through a dense layer:\n\n`x_dense = self.concat_dense(x_cat.permute(0, 2, 1)).permute(0, 2, 1)`\n\nThis combines features across the temporal axis.\n\n#### Repeated Blocks\n\nThe data is passed through the repeated blocks, each adding residual connections:\n```\nx_repeated = x_dense\nfor block in self.repeat_blocks:\n    x_repeated = x_repeated + block(x_repeated)  # Residual connection\n```\n#### Final Block\n\nThe final block further processes the data to produce the output:\n```\nx_final = self.final_relu(x_repeated)\nx_final = self.final_bn(x_final)\nx_final = self.final_conv(x_final)\n```\nThe output is transposed back to match the input format:\n\n`x_final = x_final.permute(0, 2, 1)`\n\nFinal shape:\n\n`[batch_size, sequence_length, output_dim]`\n___\n### 3. Output\nThe network outputs a tensor with the shape:\n\n```[batch_size, sequence_length, output_dim]```\nFor example, if:\n\n`batch_size = 32`\n`sequence_length = 200`\n`output_dim = 4`\n\nThe output shape will be:\n\n`[32, 200, 4]`\nEach sample in the sequence is represented by a vector of size output_dim.\n\n\n","metadata":{}},{"cell_type":"code","source":"class ConvolutionalEncoder(nn.Module):\n    def __init__(self, input_channels=1, output_dim=4, repeat_blocks=4):\n        super(ConvolutionalEncoder, self).__init__()\n        \n        # Parallel convolutional paths\n        self.conv1d_128 = nn.Sequential(\n            nn.ReflectionPad1d((63, 64)),\n            nn.Conv1d(input_channels, 100, kernel_size=128, stride=1)\n        )\n        self.conv1d_64 = nn.Sequential(\n            nn.ReflectionPad1d((31, 32)),\n            nn.Conv1d(input_channels, 100, kernel_size=64, stride=1)\n        )\n        self.conv1d_16 = nn.Sequential(\n            nn.ReflectionPad1d((7, 8)),\n            nn.Conv1d(input_channels, 50, kernel_size=16, stride=1)\n        )\n\n        # Dense layer to merge paths\n        self.concat_dense = nn.Linear(100 + 100 + 50, 250)\n\n        # Repeat N=4 blocks\n        self.repeat_blocks = nn.ModuleList([\n            nn.Sequential(\n                nn.ReLU(),\n                nn.BatchNorm1d(250),\n                nn.ReflectionPad1d((31, 32)),\n                nn.Conv1d(250, 250, kernel_size=64, stride=1)\n            ) for _ in range(repeat_blocks)\n        ])\n\n        # Final block\n        self.final_relu = nn.ReLU()\n        self.final_bn = nn.BatchNorm1d(250)\n        self.final_conv = nn.Sequential(\n            nn.ReflectionPad1d((31, 32)),\n            nn.Conv1d(250, output_dim, kernel_size=64, stride=1)\n        )\n        \n    def forward(self, x):\n        # Input shape: [batch_size, sequence_length, channels] modify\n\n        \n        x = x.permute(0, 2, 1)  #[batch_size, channels, sequence_length]\n        \n        # Parallel convolutional paths\n        x1 = self.conv1d_128(x)\n        x2 = self.conv1d_64(x)\n        x3 = self.conv1d_16(x)\n\n        # Concatenate paths\n        x_cat = torch.cat([x1, x2, x3], dim=1)  # [batch_size, 250, sequence_length]\n        \n        # Dense layer\n        x_dense = self.concat_dense(x_cat.permute(0, 2, 1)).permute(0, 2, 1)\n\n        # Repeated blocks\n        x_repeated = x_dense\n        for block in self.repeat_blocks:\n            x_repeated = x_repeated + block(x_repeated)  # Residual connection\n\n        # Final block\n        x_final = self.final_relu(x_repeated)\n        x_final = self.final_bn(x_final)\n        x_final = self.final_conv(x_final)\n        x_final = x_final.permute(0, 2, 1)\n        #print(\"xfinal:\", x_final.shape)\n        return x_final  # [batch_size, output_dim]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The **RecurrentEncoder** is a hierarchical recurrent neural network designed to process sequential data, progressively extract features, and combine hierarchical representations. It incorporates GRU layers and a residual recurrent mechanism for deeper temporal learning\n\n___\n\n#### 1. Hierarchical GRU Layers:\n\nThe architecture uses three GRU layers at different levels of abstraction:\n* **GRU (256):** Processes the input sequence at the highest dimensionality.\n* **GRU (128):** Processes downsampled features for mid-level abstraction.\n* **GRU (64):** Processes the most downsampled features for fine-grained details.\n  \nThese layers extract temporal dependencies across different feature representations.\n\n#### 2. Downsampling and Upsampling:\n\n**Downsampling:**\n\nLinear layers reduce the dimensionality between consecutive GRU layers (256 → 128 → 64).\nHelps in compressing features for more efficient learning.\n\n**Upsampling:**\n\nLinear layers increase the dimensionality back (64 → 128 → 256) to unify features at different levels.\n\n#### 3. Feature Concatenation:\n\nFeatures from all GRU layers are concatenated along the feature axis to create a comprehensive representation:\n\n* `[x_256, x_128, x_64]`.\nThis combines information from all levels of the hierarchy.\n\n#### 4. Recurrent Residual Units (RRU):\n\nA series of GRU layers with residual connections are applied to the concatenated features.\nResidual connections help in preserving the original information while learning additional temporal dependencies.\n\nEach RRU consists of:\n\nA layer normalization for stability.\nA GRU to model temporal dynamics.\n5. Output Layer:\n\nA fully connected layer maps the processed hidden representation to the final `output_dim`.\n___\n### Data Flow (forward method)\n**Input:**\nThe input x is a sequence with shape:\n\n`[batch_size, sequence_length, input_dim]`\n\n**Hierarchical Feature Extraction:**\n\n* Data flows through the hierarchical GRU layers:\n  * `x_256` (GRU with 256 units).\n  * Downsampled to `x_128_input`, then passed to GRU (128 units).\n  * Downsampled to `x_64_input`, then passed to GRU (64 units).\n* Each GRU captures temporal dependencies at its respective level.\n\n**Upsampling:**\n\nThe output from the lowest GRU level (`x_64`) is progressively upsampled to match higher-level dimensions.\n\n**Feature Concatenation:**\n\nCombine the outputs from all GRU levels (`x_256`, `x_128`, `x_64`) into a unified representation.\n\n**Residual Recurrent Processing:**\n\nThe concatenated features pass through a series of **residual GRU units** (RRU layers) to refine temporal relationships.\n\n**Output:**\nThe final representation is passed through a dense layer to map it to the desired `output_dim`.\n\nOutput shape:\n\n`[batch_size, sequence_length, output_dim]`\n","metadata":{}},{"cell_type":"code","source":"class RecurrentEncoder(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=128, repeat_n=2):\n        super(RecurrentEncoder, self).__init__()\n\n        self.gru_256 = nn.GRU(input_dim, 256, batch_first=True)\n        self.downsample_256_128 = nn.Linear(256, 128)\n        self.gru_128 = nn.GRU(128, 128, batch_first=True)\n        self.downsample_128_64 = nn.Linear(128, 64)\n        self.gru_64 = nn.GRU(64, 64, batch_first=True)\n\n        self.upsample_64_128 = nn.Linear(64, 128)\n        self.upsample_128_256 = nn.Linear(128, 256)\n\n        self.concat_dense = nn.Linear(256 + 128 + 64, hidden_dim)\n\n        self.rru = nn.ModuleList([\n            nn.Sequential(\n                nn.LayerNorm(hidden_dim),\n                nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n            ) for _ in range(repeat_n)\n        ])\n\n        self.output_dense = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x_256, _ = self.gru_256(x)\n        x_128_input = F.relu(self.downsample_256_128(x_256))\n        x_128, _ = self.gru_128(x_128_input)\n        x_64_input = F.relu(self.downsample_128_64(x_128))\n        x_64, _ = self.gru_64(x_64_input)\n\n        x_128_up = F.relu(self.upsample_64_128(x_64))\n        x_256_up = F.relu(self.upsample_128_256(x_128_up))\n\n        x_concat = torch.cat([x_256, x_128, x_64], dim=-1)\n        x_hidden = F.relu(self.concat_dense(x_concat))\n\n        for i, rru_layer in enumerate(self.rru):\n            residual, _ = rru_layer(x_hidden)\n            x_hidden = x_hidden + residual\n\n        output = self.output_dense(x_hidden)\n        return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*****\n","metadata":{}},{"cell_type":"markdown","source":"**1. EEGClassificationDataset Class**\n\nA PyTorch Dataset class for loading and preprocessing EEG data for classification tasks.\n\n**Purpose:**\n* Loads `.npz` files containing EEG data.\n* Remaps labels (`-1, 0, 1` → `0, 1, 2`) for multi-class classification.\n* Transposes data to have the shape `(num_samples, seq_len, num_channels)`.\n\n**Output:**\nReturns a sequence (`data`) and its corresponding label for training.\n\n**2. Classifier Class**\nA neural network for classifying EEG data using hierarchical Bidirectional LSTMs (BiLSTMs).\n\nArchitecture:\nThree stages of dimensionality reduction using `Linear` layers followed by BiLSTMs:\n* `input_dim → 256 → 128 → 64`.\n  \n* Final dense layers combine features and produce classification logits.\n* Uses `LogSoftmax` for probabilistic outputs.\n\n\n**Output:**\n\nLog-probabilities for each class.\n\n**3. load_model and save_model Functions**\n\n* **Purpose:** Save and load PyTorch models.\n* Handles models trained with `DataParallel` by removing the `module.` prefix from saved state dictionaries.\n  \n**4. train_classifier Function**\n\nThe training loop for optimizing the classifier.\n\n**Workflow:**\n\n* **Dataset and Dataloader:**\n  \nLoads EEG data using EEGClassificationDataset.\nCreates batches of data with a specified batch size.\n\n**Encoder Freeze:**\n\nFreezes the pretrained encoder to prevent updates during classifier training.\n\n**Forward Pass:**\n\nProcesses each channel of the data through the encoder (pretrained).\n\nConcatenates embeddings from all channels.\n\nPasses concatenated embeddings through the classifier.\n\n**Loss and Optimization:**\n\nUses Negative Log-Likelihood Loss (NLLLoss) for classification.\nOptimizes the classifier using backpropagation.\n\n**Metrics and Logging:**\n\nTracks the loss and accuracy per batch and epoch.\nLogs intermediate metrics for monitoring.\n\n**Model Checkpoints:**\n\nSaves the classifier model at the end of each epoch for reproducibility.","metadata":{}},{"cell_type":"code","source":"class EEGClassificationDataset(Dataset):\n    def __init__(self, file_paths, seq_len=200):\n        self.samples = []\n        self.labels = []\n        self.seq_len = seq_len\n\n        for file_path in file_paths:\n            npz_data = np.load(file_path)\n            data = npz_data['data']  # (num_samples, num_channels, seq_len)\n            label = npz_data['label']  # scalare\n\n            if isinstance(label, np.ndarray):\n                label = label.item()\n\n            # label potrebbe essere -1, 0 o 1. Li rimappiamo in 0, 1, 2.\n            if label == -1:\n                label = 0\n            elif label == 0:\n                label = 1\n            elif label == 1:\n                label = 2\n\n            labels = np.full((data.shape[0],), label)\n\n            # Cambia la forma a (num_samples, seq_len, num_channels)\n            data = data.transpose(0, 2, 1)\n\n            self.samples.extend(data)\n            self.labels.extend(labels)\n\n        class_counts = Counter(self.labels)\n        print(\"Distribuzione delle classi nel dataset:\", class_counts)\n        print(\"Dataset creato con\", len(self.samples), \"segmenti.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        data = torch.tensor(self.samples[idx], dtype=torch.float32)  # (seq_len, num_channels)\n        label = int(self.labels[idx])\n        return data, label\n\n\nclass Classifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(Classifier, self).__init__()\n\n        self.downsample_1 = nn.Linear(input_dim, 256)\n        self.bilstm_256 = nn.LSTM(256, 128, batch_first=True, bidirectional=True)\n\n        self.downsample_2 = nn.Linear(256, 128)\n        self.bilstm_128 = nn.LSTM(128, 64, batch_first=True, bidirectional=True)\n\n        self.downsample_3 = nn.Linear(128, 64)\n        self.bilstm_64 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n\n        self.concat_dense_1 = nn.Linear(256 + 128 + 64, 128)\n        self.concat_dense_2 = nn.Linear(128, num_classes)\n\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        x_256 = F.relu(self.downsample_1(x))\n        x_256, (h_256, _) = self.bilstm_256(x_256)\n        flo_256 = torch.cat([h_256[0], h_256[1]], dim=-1)\n\n        x_128 = F.relu(self.downsample_2(x_256))\n        x_128, (h_128, _) = self.bilstm_128(x_128)\n        flo_128 = torch.cat([h_128[0], h_128[1]], dim=-1)\n\n        x_64 = F.relu(self.downsample_3(x_128))\n        x_64, (h_64, _) = self.bilstm_64(x_64)\n        flo_64 = torch.cat([h_64[0], h_64[1]], dim=-1)\n\n        x_concat = torch.cat([flo_256, flo_128, flo_64], dim=-1)\n        x_hidden = F.relu(self.concat_dense_1(x_concat))\n        output = self.concat_dense_2(x_hidden)\n        return self.log_softmax(output)\n\ndef load_model(model, path, device=\"cuda\"):\n    state_dict = torch.load(path, map_location=device)\n    # Rimuoviamo eventuale prefisso 'module.' se salvato con DataParallel\n    if any(k.startswith('module.') for k in state_dict.keys()):\n        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n    model.load_state_dict(state_dict)\n    model.to(device)\n\ndef save_model(model, path):\n    if isinstance(model, nn.DataParallel):\n        model = model.module\n    torch.save(model.state_dict(), path)\n\ndef train_classifier(model_type, file_paths, encoder, classifier, optimizer, epochs=1, batch_size=40, device=\"cuda\"):\n    dataset = EEGClassificationDataset(file_paths)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\n    encoder.eval()\n    for param in encoder.parameters():\n        param.requires_grad = False\n    encoder = nn.DataParallel(encoder).to(device)\n\n    classifier = nn.DataParallel(classifier).to(device)\n    classifier.train()\n    loss_fn = nn.NLLLoss()\n\n    save_path = \"./model_checkpoints\"\n    os.makedirs(save_path, exist_ok=True)\n\n    for epoch in range(epochs):\n        print(f\"=== Inizio Epoca {epoch + 1}/{epochs} ===\")\n        total_loss = 0\n        correct_predictions = 0\n        total_predictions = 0\n\n        for batch_idx, (data, labels) in enumerate(dataloader):\n            #print(f\"Batch {batch_idx + 1}: Shape data: {data.shape}, Shape labels: {labels.shape}\")\n            data, labels = data.to(device), labels.to(device)\n\n            batch_outputs = []\n            # Itera sui canali\n            for channel_idx in range(data.shape[2]):\n                channel_data = data[:, :, channel_idx].unsqueeze(-1)  # (batch_size, seq_len, 1)\n                with torch.no_grad():\n                    channel_output = encoder(channel_data)  # (batch_size, seq_len, 4)\n                batch_outputs.append(channel_output)\n\n            # Concateniamo le feature di tutti i canali\n            embeddings = torch.cat(batch_outputs, dim=-1)  # (batch_size, seq_len, 4*num_channels)\n            #print(\"Embeddings shape:\", embeddings.shape)\n\n            logits = classifier(embeddings)\n            loss = loss_fn(logits, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, predicted = torch.max(logits, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_predictions += labels.size(0)\n\n            if batch_idx % 10 == 0:\n                accuracy = 100 * correct_predictions / total_predictions\n                print(f\"Epoca {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(dataloader)}, Perdita: {loss.item():.4f}, Accuratezza: {accuracy:.2f}%\")\n\n        epoch_accuracy = 100 * correct_predictions / total_predictions\n        print(f\"Epoca {epoch + 1}/{epochs}, Perdita Totale: {total_loss:.4f}, Accuratezza: {epoch_accuracy:.2f}%\")\n        save_model(classifier, os.path.join(save_path, f\"classifier_{model_type}_epoch_{epoch + 1}.pt\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    folder_path = \"/kaggle/working/train_set\"\n    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.npz')]\n\n    encoder = RecurrentEncoder(input_dim=1, output_dim=4)\n    load_model(encoder, \"/kaggle/input/recurrent_encoder_/pytorch/default/1/Recurrent_epoch_24.pth\", device=\"cuda\")\n\n    # Il numero di canali è 20 e l'encoder produce 4 feature per canale: input_dim=4*num_channels=4*20=80\n    classifier = Classifier(input_dim=4 * 20, num_classes=3)\n    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n\n    train_classifier(\"Recurrent\", file_paths, encoder, classifier, optimizer, epochs=10, batch_size=40, device=\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    folder_path = \"/kaggle/working/train_set\"\n    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.npz')]\n\n    encoder = ConvolutionalEncoder(input_channels=1, output_dim=4)\n    load_model(encoder, \"/kaggle/input/convolutional_encoder/pytorch/default/1/Convolutional_epoch_10.pth\", device=\"cuda\")\n\n    # Il numero di canali è 20 e l'encoder produce 4 feature per canale: input_dim=4*num_channels=4*20=80\n    classifier = Classifier(input_dim=4 * 20, num_classes=3)\n    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n\n    train_classifier(\"Convolutional\", file_paths, encoder, classifier, optimizer, epochs=10, batch_size=40, device=\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EVALUATION**","metadata":{}},{"cell_type":"markdown","source":"**1. EEGEvaluationDataset Class**\n\nThis PyTorch Dataset class prepares EEG test data for evaluation.\n\n**Key Features:**\n\nLoads and processes EEG data:\nReads `.npz` files containing EEG data (`data`) and their associated labels (`label`).\n\n**Label remapping:**\n\n* Remaps labels:\n* `-1 → 0` (negative).\n* `0 → 1` (neutral).\n* `1 → 2` (positive).\n* \n**Transposes data:**\n  \nConverts data shape from `(num_samples, num_channels, seq_len)` to `(num_samples, seq_len, num_channels)` for compatibility with PyTorch models.\n**Output:**\n\nEach sample (`data`) and its label are returned during iteration.\n\n**2. Classifier Class**\n\nA neural network for EEG signal classification.\n\n**Architecture:**\n\n**Hierarchical BiLSTM Blocks:**\n\n* Three stages:\n\n* Dimensionality reduction with `Linear` layers.\n* Temporal feature extraction with bidirectional LSTMs (BiLSTMs).\n* Dimensions reduced as: `input_dim → 256 → 128 → 64`.\n\n**Feature Concatenation:**\nCombines outputs from all three stages into a single feature vector.\n\n**Final Classification:**\n\nDense layers map the concatenated features to class logits.\nLogSoftmax provides probabilistic outputs for classification.\n\n**Output:**\n\nLog-probabilities for each class.\n\n**3. Utility Functions**\n\n**load_model:**\n\n* Loads a pre-trained model from a file.\n\n* Removes any module. prefixes in the state dictionary for compatibility with single/multi-GPU setups.\n\n**evaluate_classifier:**\n\nEvaluates the encoder and classifier on a test dataset.\n\n**4. evaluate_classifier Function**\n\nThe main function that evaluates the classifier using the test dataset.\n\n**Workflow:**\n\n**Dataset and Dataloader:**\n\n* Loads EEG test data using the `EEGEvaluationDataset` class.\n* Creates a dataloader to iterate through the test samples.\n**Model Evaluation:**\n  \n* Sets both the encoder and classifier to evaluation mode (`eval()`).\n* Freezes gradients (`torch.no_grad()`).\n\n**Forward Pass:**\n\nProcesses data through the encoder for each channel:\n* Each channel is passed individually through the encoder.\n* Channel-wise embeddings are concatenated.\nThe concatenated embeddings are passed through the classifier to generate logits.\n\n**Metrics Calculation:**\n\nPredictions (`all_preds`) and true labels (`all_labels`) are stored.\nComputes accuracy, confusion matrix, and classification report.\n**5. Outputs**\n\n**Accuracy:**\n\nOverall accuracy of the classifier on the test set.\n\n**Confusion Matrix:**\n\nDisplays the number of true positives, false positives, and false negatives for each class.\n\n**Classification Report:**\n\nPrecision, recall, and F1-score for each class.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nclass EEGEvaluationDataset(Dataset):\n    def __init__(self, file_paths, seq_len=200):\n        self.samples = []\n        self.labels = []\n        self.seq_len = seq_len\n\n        for file_path in file_paths:\n            npz_data = np.load(file_path)\n            data = npz_data['data']  # (num_samples, num_channels, seq_len)\n            label = npz_data['label']  \n\n            if isinstance(label, np.ndarray):\n                label = label.item()\n\n            if label == -1:\n                label = 0\n            elif label == 0:\n                label = 1\n            elif label == 1:\n                label = 2\n\n            labels = np.full((data.shape[0],), label)\n\n            # Cambia forma a (num_samples, seq_len, num_channels)\n            data = data.transpose(0, 2, 1)\n\n            self.samples.extend(data)\n            self.labels.extend(labels)\n\n        class_counts = Counter(self.labels)\n        print(\"Distribuzione delle classi nel dataset (test):\", class_counts)\n        print(\"Test set creato con\", len(self.samples), \"segmenti.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        data = torch.tensor(self.samples[idx], dtype=torch.float32)  # (seq_len, num_channels)\n        label = int(self.labels[idx])\n        return data, label\n\nclass Classifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(Classifier, self).__init__()\n\n        self.downsample_1 = nn.Linear(input_dim, 256)\n        self.bilstm_256 = nn.LSTM(256, 128, batch_first=True, bidirectional=True)\n\n        self.downsample_2 = nn.Linear(256, 128)\n        self.bilstm_128 = nn.LSTM(128, 64, batch_first=True, bidirectional=True)\n\n        self.downsample_3 = nn.Linear(128, 64)\n        self.bilstm_64 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n\n        self.concat_dense_1 = nn.Linear(256 + 128 + 64, 128)\n        self.concat_dense_2 = nn.Linear(128, num_classes)\n\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        x_256 = F.relu(self.downsample_1(x))\n        x_256, (h_256, _) = self.bilstm_256(x_256)\n        flo_256 = torch.cat([h_256[0], h_256[1]], dim=-1)\n\n        x_128 = F.relu(self.downsample_2(x_256))\n        x_128, (h_128, _) = self.bilstm_128(x_128)\n        flo_128 = torch.cat([h_128[0], h_128[1]], dim=-1)\n\n        x_64 = F.relu(self.downsample_3(x_128))\n        x_64, (h_64, _) = self.bilstm_64(x_64)\n        flo_64 = torch.cat([h_64[0], h_64[1]], dim=-1)\n\n        x_concat = torch.cat([flo_256, flo_128, flo_64], dim=-1)\n        x_hidden = F.relu(self.concat_dense_1(x_concat))\n        output = self.concat_dense_2(x_hidden)\n        return self.log_softmax(output)\n\ndef load_model(model, path, device=\"cuda\"):\n    state_dict = torch.load(path, map_location=device)\n    if any(k.startswith('module.') for k in state_dict.keys()):\n        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n    model.load_state_dict(state_dict)\n    model.to(device)\n\ndef evaluate_classifier(file_paths, encoder, classifier,batch_size=40, device=\"cuda\"):\n    dataset = EEGEvaluationDataset(file_paths)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n\n    encoder.eval()\n    classifier.eval()\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for data, labels in dataloader:\n            data, labels = data.to(device), labels.to(device)\n\n            batch_outputs = []\n            for channel_idx in range(data.shape[2]):\n                channel_data = data[:, :, channel_idx].unsqueeze(-1)  # (batch_size, seq_len, 1)\n                channel_output = encoder(channel_data)  # (batch_size, seq_len, 4)\n                batch_outputs.append(channel_output)\n\n            embeddings = torch.cat(batch_outputs, dim=-1)  # (batch_size, seq_len, 4*num_channels)\n            logits = classifier(embeddings)\n            _, predicted = torch.max(logits, 1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calcolo metriche\n    acc = accuracy_score(all_labels, all_preds)\n    cm = confusion_matrix(all_labels, all_preds)\n    report = classification_report(all_labels, all_preds, digits=4)\n\n    print(\"=== Risultati sul Test Set ===\")\n    print(\"Accuratezza:\", acc)\n    print(\"Matrice di Confusione:\")\n    print(cm)\n    print(\"Report di Classificazione (Precision, Recall, F1-Score):\")\n    print(report)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    test_folder_path = \"/kaggle/working/test_set\"\n    test_file_paths = [os.path.join(test_folder_path, f) for f in os.listdir(test_folder_path) if f.endswith('.npz')]\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n    encoder = RecurrentEncoder(input_dim=1, output_dim=4)  \n    classifier = Classifier(input_dim=4 * 20, num_classes=3)\n\n    load_model(encoder, \"/kaggle/input/recurrent_encoder_/pytorch/default/1/Recurrent_epoch_24.pth\", device=device)\n    load_model(classifier, \"/kaggle/input/classifier_recurrent/pytorch/default/1/classifier_Recurrent_epoch_10.pt\", device=device)\n\n    evaluate_classifier(test_file_paths, encoder, classifier,batch_size=40, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    test_folder_path = \"/kaggle/working/test_set\"\n    test_file_paths = [os.path.join(test_folder_path, f) for f in os.listdir(test_folder_path) if f.endswith('.npz')]\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Carica i modelli già addestrati\n    encoder = ConvolutionalEncoder(input_channels=1, output_dim=4) \n    classifier = Classifier(input_dim=4 * 20, num_classes=3)  \n\n    load_model(encoder, \"/kaggle/input/convolutional_encoder/pytorch/default/1/Convolutional_epoch_10.pth\", device=device)\n    load_model(classifier, \"/kaggle/input/classifier_convolutional/pytorch/default/1/classifier_Convolutional_epoch_10.pt\", device=device)\n\n    evaluate_classifier(test_file_paths, encoder, classifier,batch_size=40, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}