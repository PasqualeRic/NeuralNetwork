{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10190077,"sourceType":"datasetVersion","datasetId":6295778},{"sourceId":10190185,"sourceType":"datasetVersion","datasetId":6295862},{"sourceId":10196847,"sourceType":"datasetVersion","datasetId":6300532},{"sourceId":199605,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":170258,"modelId":192581}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contrastive Representation Learning for Electroencephalogram Classification\n\nAuthor: Pasquale Ricciulli (2115446)","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport glob\nfrom scipy.signal import butter, filtfilt\nfrom scipy.signal import resample_poly\nimport librosa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:02:01.735119Z","iopub.execute_input":"2024-12-13T11:02:01.735348Z","iopub.status.idle":"2024-12-13T11:02:01.740465Z","shell.execute_reply.started":"2024-12-13T11:02:01.735323Z","shell.execute_reply":"2024-12-13T11:02:01.739285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The code reduces the size of a preprocessed EEG dataset in .mat (MATLAB) format by applying two main strategies:\n___\n\n* **Trial Reduction:** \n\n    * A random subset of trials is selected (one-third of the total trials per file).\n\n* **Data Reduction:**\n\n    * Number of Channels: Reduced to one-third.\n    * Number of Samples: Reduced to half.\nThe resulting dataset is saved in .npz format (compatible with NumPy).","metadata":{}},{"cell_type":"code","source":"#riduzione dataset seed\nimport os\nimport numpy as np\nfrom scipy.io import loadmat\nimport random\n\n# Percorsi\ninput_path = '/kaggle/input/seed-dataset/Preprocessed_EEG'\noutput_path = '/kaggle/working/reduced_dataset'\n\n# Crea la cartella di output\nos.makedirs(output_path, exist_ok=True)\n\n# Parametri per la riduzione\nchannel_reduction_ratio = 3  # Dividi il numero di canali per 3\nsample_reduction_ratio = 2   # Dividi il numero di campioni per 2\n\n# Elenca tutti i file .mat\nmat_files = [f for f in os.listdir(input_path) if f.endswith('.mat') and f != 'label.mat']\nprint(f\"Trovati {len(mat_files)} file .mat nella directory.\")\n\n# Itera su tutti i file\nfor file_name in mat_files:\n    file_path = os.path.join(input_path, file_name)\n    print(f\"Processando il file: {file_name}\")\n    \n    # Carica il file .mat\n    mat_data = loadmat(file_path)\n    \n    # Trova tutte le chiavi che rappresentano trial (chiavi con strutture comuni come djc_eeg, ys_eeg, ecc.)\n    trial_keys = [key for key in sorted(mat_data.keys()) if not key.startswith('__') and isinstance(mat_data[key], np.ndarray)]\n    print(f\"Trial trovati in {file_name}: {trial_keys}\")\n    \n    if not trial_keys:\n        print(f\"Nessun trial trovato in {file_name}, salto il file.\")\n        continue\n    \n    # Seleziona casualmente un terzo dei trial\n    random.seed(42)  # Per replicabilità\n    selected_keys = random.sample(trial_keys, len(trial_keys) // 3)\n    \n    for key in selected_keys:  # Itera SOLO sui trial selezionati\n        data = mat_data[key]\n        \n        # Riduzione del numero di canali\n        num_channels = data.shape[0]\n        reduced_channels = num_channels // channel_reduction_ratio\n        reduced_data = data[:reduced_channels, :]\n        \n        # Riduzione del numero di campioni\n        num_samples = reduced_data.shape[1]\n        reduced_samples = num_samples // sample_reduction_ratio\n        reduced_data = reduced_data[:, :reduced_samples]\n        \n        # Salva i dati ridotti\n        output_file = os.path.join(output_path, f\"{file_name.replace('.mat', '')}_{key}.npz\")\n        np.savez(output_file, data=reduced_data)\n\nprint(f\"Dataset ridotto salvato in: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:02:01.945117Z","iopub.execute_input":"2024-12-13T11:02:01.945399Z","iopub.status.idle":"2024-12-13T11:04:25.576258Z","shell.execute_reply.started":"2024-12-13T11:02:01.945372Z","shell.execute_reply":"2024-12-13T11:04:25.575378Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code preprocesses an EEG dataset saved in .npz format, performing the following steps:\n___\n**Setup:**\nReads all .npz files from the specified input folder (/kaggle/working/reduced_dataset).\n\nCreates an output folder (/kaggle/working/preprocessed) to save the processed data.\n\n**Processing Loop:**\n\nIterates over all .npz files:\n\n* Loads the data and extracts arrays for each trial (key) in the file.\n**For each trial:**\n* Processes each channel by:\n* Identifying extreme values (above 500 µV) and replacing them with 0.\n* Saves the cleaned, processed data as a NumPy array (.npy) in the output folder.\n\n**Error Handling:**\n\nIf any file encounters issues during processing, an error message is printed, but the loop continues with the next file.\n\n**Output:**\n\nProcessed EEG data is saved in .npy format in the /kaggle/working/preprocessed folder.\n","metadata":{}},{"cell_type":"code","source":"#preprocessamento dataset seed\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter, filtfilt\nfrom scipy.io import loadmat\nimport warnings\n\n# Percorso dei file\nfolder_path = '/kaggle/working/reduced_dataset'\nfile_paths = glob.glob(f\"{folder_path}/*.npz\")\nfolder1_path = '/kaggle/working/preprocessed'\nos.makedirs(folder1_path, exist_ok=True)\n# Ignora i warning\nwarnings.filterwarnings(\"ignore\")\n\n# Funzione per plottare i segnali\ndef plot_signals(original_signal, cleaned_signal, channel_idx, file_name):\n    plt.figure(figsize=(15, 10))\n\n    plt.subplot(2, 1, 1)\n    plt.plot(original_signal, label='Originale', color='blue')\n    plt.title(f\"{file_name} - Canale {channel_idx} (Originale)\")\n    plt.xlabel(\"Campioni\")\n    plt.ylabel(\"Ampiezza\")\n    plt.legend()\n\n    plt.subplot(2, 1, 2)\n    plt.plot(cleaned_signal, label='Pulito (±500 µV)', color='red')\n    plt.title(f\"{file_name} - Canale {channel_idx} (Pulito)\")\n    plt.xlabel(\"Campioni\")\n    plt.ylabel(\"Ampiezza\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n# Elaborazione file\nfor file_path in file_paths:\n    try:\n        # Carica il file .mat\n        file_name = os.path.basename(file_path).replace('.npz', '')\n        print(f\"Processando il file: {file_name}\")\n\n        npz_data = np.load(file_path)\n        \n        # Itera sulle chiavi del file .npz\n        for key in npz_data.keys():\n            data = npz_data[key]\n            print(f\"Elaborazione della chiave: {key} - Forma: {data.shape}\")\n            \n            # Salva i dati in formato .npy\n\n            # Preprocessing dei canali\n            processed_channels = []\n            for channel_idx in range(data.shape[0]):\n                original_signal = data[channel_idx, :]\n\n                # Rimozione valori superiori a ±500 µV\n                cleaned_signal = original_signal.copy()\n                cleaned_signal[np.abs(cleaned_signal) > 500] = 0  # Sostituisci con 0\n\n                # Plot dei segnali\n                #plot_signals_all_channels(original_signal, cleaned_signal, channel_idx, file_name)\n\n                processed_channels.append(cleaned_signal)\n\n            processed_data = np.array(processed_channels)\n            print(f\"Dati processati per la chiave {key}: forma {processed_data.shape}\")\n\n            # Salva i dati processati in formato .npy\n            output_path = os.path.join(folder1_path, f\"processed_{file_name}_{key}.npy\")\n            np.save(output_path, processed_data)\n            print(f\"Dati salvati in formato NumPy: {output_path}\")\n\n    except Exception as e:\n        print(f\"Errore durante il preprocessing del file {file_path}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:40:37.720081Z","iopub.execute_input":"2024-12-13T11:40:37.720965Z","iopub.status.idle":"2024-12-13T11:40:39.875984Z","shell.execute_reply.started":"2024-12-13T11:40:37.720927Z","shell.execute_reply":"2024-12-13T11:40:39.875024Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FASE DI PREPROCESSING E REORDERING DEI FILE IN COLONNE**","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"#### Setup\n   \nReads all .txt files from a specified folder.\n\n* Defines parameters for:\n    * Resampling: Converts signals to a target sampling frequency (200 Hz) from an original frequency of 256 Hz.\n    * Filtering: Applies a Butterworth band-pass filter to retain frequencies between 0.3 Hz and 80 Hz.\n\n* Artifact Removal:\n\n    * Removes values exceeding ±500 µV, replacing them with NaN.\n    * Uses interpolation to fill gaps caused by artifact removal, and fills remaining gaps with 0.\n\n#### Processing Pipeline\n\nFor each file:\n\n* **Load Data:**\n\nReads EEG signals from the .txt file using pandas (pd.read_csv). Each column represents a channel.\n* **Channel-wise Processing:**\n\nIterates over each channel in the file and performs:\n* **Resampling:**\nUses librosa for high-quality resampling to 200 Hz.\nHandles missing or infinite values (NaN/Inf) by replacing them with 0.\n* **Filtering:**\nApplies a Butterworth band-pass filter to remove noise and retain relevant frequencies.\n\n* **Artifact Removal:**\n\nRemoves values exceeding ±500 µV, replacing them with NaN.\nUses interpolation to fill gaps caused by artifact removal, and fills remaining gaps with 0.\n* **Combine Processed Channels:**\nAll processed channels are combined into a single NumPy array (cleaned_data).\n\n#### Output\nThe processed signals are saved in .npy format, with one file per original .txt file.\nEach .npy file contains the cleaned, filtered, and resampled data for all channels of the corresponding EEG signal.","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter, filtfilt\nfrom scipy.io import savemat\nimport librosa\n\n# Funzione per il filtro band-pass Butterworth\ndef butter_bandpass(lowcut, highcut, fs, order=5):\n    nyquist = 0.5 * fs\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\n# Funzione per applicare il filtro ai dati\ndef butter_filter(data, lowcut, highcut, fs, order=5):\n    b, a = butter_bandpass(lowcut, highcut, fs, order)\n    return filtfilt(b, a, data)\n\n# Funzione per il resampling usando librosa\ndef librosa_resample(data, original_fs, target_fs):\n    # Controlla e gestisce NaN o inf nel segnale\n    data = np.nan_to_num(data)  # Sostituisci NaN con 0 e inf con valori finiti\n    return librosa.resample(data, orig_sr=original_fs, target_sr=target_fs, res_type='soxr_hq')\n\n# Percorso alla cartella contenente i file .txt\nfolder_path = '/kaggle/input/dataverse-files/dataverse_files'\nfolder1_path = '/kaggle/working/preprocessed'\n#os.makedirs(folder1_path, exist_ok=True)\n\n# Lista di tutti i file .txt nella cartella\nfile_paths = glob.glob(os.path.join(folder_path, '*.txt'))\n\n# Parametri di frequenza di campionamento target\ntarget_fs = 200  # Frequenza di campionamento desiderata\noriginal_fs = 256\n# Parametri per il filtro Butterworth\nlowcut = 0.3\nhighcut = 80.0\n\n# Durata del segnale in secondi\nsignal_duration = 20  # Preso dal paper, scelgono dataset con 20 sec\n\nfor file_path in file_paths:\n    # Estrai il nome del file per identificazione\n    file_name = os.path.basename(file_path).replace('.txt', '')\n    print(f\"Processando il file: {file_name}\")\n\n    # Carica il file .txt\n    data = pd.read_csv(file_path, sep='\\t', header=None)\n\n    cleaned_signals = []\n    for channel_idx in range(data.shape[1]):  # Itera su tutti i canali\n        original_signal = data.iloc[:, channel_idx].values  # Estrarre il segnale come array NumPy\n\n        # Rimozione di eventuali valori non numerici prima del resampling\n        original_signal = np.nan_to_num(original_signal)\n\n        # Resampling usando librosa\n        resampled_signal = librosa_resample(original_signal, original_fs, target_fs)\n\n        # Filtraggio\n        filtered_signal = butter_filter(resampled_signal, lowcut, highcut, target_fs)\n\n        # Rimozione valori ±500 µV\n        cleaned_signal = filtered_signal.copy()\n        cleaned_signal[np.abs(cleaned_signal) > 500] = np.nan  # Sostituisci con NaN\n        cleaned_signal = pd.Series(cleaned_signal).interpolate().fillna(0).values  # Interpolazione per riempire i vuoti\n        cleaned_signals.append(cleaned_signal)\n    # Converte i canali preprocessati in un array NumPy\n    cleaned_data = np.array(cleaned_signals)\n    print(f\"Dati processati forma {cleaned_data.shape}\")\n    # Salva i dati preprocessati in formato .mat\n    output_path = os.path.join(folder1_path, f\"preprocessing_{file_name}.npy\")\n    np.save(output_path, cleaned_data)\n    print(f\"File con preprocessing salvato come npy: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:34:56.827313Z","iopub.execute_input":"2024-12-13T11:34:56.827681Z","iopub.status.idle":"2024-12-13T11:35:01.960507Z","shell.execute_reply.started":"2024-12-13T11:34:56.827649Z","shell.execute_reply":"2024-12-13T11:35:01.959301Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------- Funzioni per Salvare e Caricare Modelli -------------------\nsave_path = \"/kaggle/working/models\"\nos.makedirs(save_path, exist_ok=True)\n\ndef save_model(model, path):\n    torch.save(model.state_dict(), path)\n    print(f\"Modello salvato in {path}\")\n\n\n#tolgo il module \ndef load_model(model, path, device=\"cuda\"):\n    state_dict = torch.load(path, map_location=device)\n    if \"module.\" in list(state_dict.keys())[0]:\n        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n    model.load_state_dict(state_dict)\n    print(f\"Modello caricato da {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:35:04.231602Z","iopub.execute_input":"2024-12-13T11:35:04.231915Z","iopub.status.idle":"2024-12-13T11:35:04.238329Z","shell.execute_reply.started":"2024-12-13T11:35:04.231889Z","shell.execute_reply":"2024-12-13T11:35:04.237506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FASE DI TRAINING DELL'ENCODER RICORRENTE E CONVOLUZIONALE**","metadata":{}},{"cell_type":"markdown","source":"This code performs data augmentation and chunking for EEG data, and includes functionality for extracting labels based on file names.\n___\n\n#### Augmentation Functions\n\nThese functions apply transformations to augment the EEG data, increasing variability and robustness for downstream tasks.\n\n* min_max_amplitude_scale(data, scale_min=0.5, scale_max=2):\n  Scales the amplitude of the signal by a random factor between 0.5 and 2.\n* time_shift(data, shift_min=-50, shift_max=50):\n  Shifts the signal in time by a random number of samples within the range [-50, 50].\n* dc_shift(data, shift_min=-10, shift_max=10):\n  Adds a constant (DC shift) to the signal, chosen randomly between -10 and 10.\n* zero_masking(data, mask_min=0, mask_max=150):\n  Masks a random segment of the signal (sets values to 0) with a random length between 0 and 150 samples.\n* add_gaussian_noise(data, sigma_min=0, sigma_max=0.2):\n  Adds Gaussian noise to the signal, with a standard deviation chosen randomly between 0 and 0.2.","metadata":{}},{"cell_type":"code","source":"#AUGMENTATION E CHUNK DATI\ndef min_max_amplitude_scale(data, scale_min=0.5, scale_max=2):\n    scale_factor = random.uniform(scale_min, scale_max)\n    return data * scale_factor\n\ndef time_shift(data, shift_min=-50, shift_max=50):\n    shift_samples = random.randint(shift_min, shift_max)\n    return np.roll(data, shift_samples)\n\ndef dc_shift(data, shift_min=-10, shift_max=10):\n    shift_value = random.uniform(shift_min, shift_max)\n    return data + shift_value\n\ndef zero_masking(data, mask_min=0, mask_max=150):\n    mask_size = random.randint(mask_min, mask_max)\n    start_idx = random.randint(0, len(data) - mask_size)\n    data[start_idx:start_idx+mask_size] = 0\n    return data\n\ndef add_gaussian_noise(data, sigma_min=0, sigma_max=0.2):\n    sigma = random.uniform(sigma_min, sigma_max)\n    noise = np.random.normal(0, sigma, len(data))\n    return data + noise\n\ndef apply_random_transformations_class(channel_data):\n    transformations = [min_max_amplitude_scale, time_shift, dc_shift, zero_masking, add_gaussian_noise]\n    selected_transform = random.choice(transformations)\n    transformed_data = selected_transform(channel_data.copy())\n    return transformed_data\n    \ndef apply_random_transformations(channel_data):\n    transformations = [min_max_amplitude_scale, time_shift, dc_shift, zero_masking, add_gaussian_noise]\n    selected_transforms = random.sample(transformations, 2)\n    transformed_data_1 = selected_transforms[0](channel_data.copy())\n    transformed_data_2 = selected_transforms[1](channel_data.copy())\n    return transformed_data_1, transformed_data_2\n    \n    \ndef chunk_data(data, chunk_size=4000):\n    \"\"\"Divide i dati in chunk della dimensione specificata.\"\"\"\n    chunks = []\n    num_chunks = len(data) // chunk_size\n\n    for i in range(num_chunks):\n        start_idx = i * chunk_size\n        end_idx = (i + 1) * chunk_size\n        chunks.append(data[start_idx:end_idx])\n    \n    return chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:35:05.753066Z","iopub.execute_input":"2024-12-13T11:35:05.753799Z","iopub.status.idle":"2024-12-13T11:35:05.764610Z","shell.execute_reply.started":"2024-12-13T11:35:05.753750Z","shell.execute_reply":"2024-12-13T11:35:05.763637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code implements a **Convolutional Encoder** designed to process sequential data, such as EEG signals.\n___\n### 1. Constructor (__init__)\n\nThe constructor defines the layers and operations of the network. The network consists of three main parts: parallel convolutional paths, repeated blocks, and a final block.\n\n#### Parallel Convolutional Paths\nThree convolutional paths operate in parallel, each capturing features at different temporal scales:\n\nConv1D with kernel size 128:\n\nAdds reflective padding to preserve the original sequence length.\nApplies 100 convolutional filters to extract features over a large temporal window.\n\nConv1D with kernel size 64:\n\nSimilar to the first, but with a smaller kernel size, capturing mid-range temporal dependencies.\n\nConv1D with kernel size 16:\n\nUses a smaller kernel size to capture fine-grained temporal features.\nThese parallel paths allow the network to process the signal at multiple scales simultaneously.\n\n#### Dense Layer for Merging\n\nAfter the parallel paths, their outputs are concatenated and combined through a fully connected (Linear) layer:\n\nMerges the 250 features (100+100+50) into a unified representation.\n#### Repeated Blocks\n\nThe encoder includes 4 repeated blocks for deeper processing of the signal. Each block consists of:\n\nA ReLU activation for non-linearity.\nBatch normalization to stabilize training.\nA Conv1D layer with kernel size 64 and 250 filters.\nResidual connections are employed: each block adds its input to the output, allowing for efficient gradient flow and preventing vanishing gradients.\n\n#### Final Block\n\nThe final layer reduces the dimensionality of the output:\n\nApplies another convolution with a kernel size of 64 and output_dim filters.\nReLU activation and batch normalization ensure stability and non-linearity.\n___\n### 2. forward: Data Flow\nThe forward method defines how the data flows through the network.\n\n#### Input\n\nThe input is a batch of sequences with the shape:\n`\n[batch_size, sequence_length, channels]\n`\n\nFor example, for EEG data with 1 channel, 200 samples per sequence, and a batch size of 32:\n`\n[32, 200, 1]\n`\n#### Transpose Input\n\nThe input is transposed to match PyTorch's Conv1d requirements:\n\n`x = x.permute(0, 2, 1)`\n\nNew shape:\n\n`[batch_size, channels, sequence_length]`\n\n### Parallel Convolutional Paths\n\nThe data passes through the three parallel convolutional paths:\n```\nx1 = self.conv1d_128(x)  # [batch_size, 100, sequence_length]\nx2 = self.conv1d_64(x)   # [batch_size, 100, sequence_length]\nx3 = self.conv1d_16(x)   # [batch_size, 50, sequence_length]\n```\n#### Concatenate Outputs\n\nThe outputs of the three paths are concatenated along the channel axis:\n\n`x_cat = torch.cat([x1, x2, x3], dim=1)  # [batch_size, 250, sequence_length]`\n\n#### Dense Layer\n\nThe concatenated features are passed through a dense layer:\n\n`x_dense = self.concat_dense(x_cat.permute(0, 2, 1)).permute(0, 2, 1)`\n\nThis combines features across the temporal axis.\n\n#### Repeated Blocks\n\nThe data is passed through the repeated blocks, each adding residual connections:\n```\nx_repeated = x_dense\nfor block in self.repeat_blocks:\n    x_repeated = x_repeated + block(x_repeated)  # Residual connection\n```\n#### Final Block\n\nThe final block further processes the data to produce the output:\n```\nx_final = self.final_relu(x_repeated)\nx_final = self.final_bn(x_final)\nx_final = self.final_conv(x_final)\n```\nThe output is transposed back to match the input format:\n\n`x_final = x_final.permute(0, 2, 1)`\n\nFinal shape:\n\n`[batch_size, sequence_length, output_dim]`\n___\n### 3. Output\nThe network outputs a tensor with the shape:\n\n```[batch_size, sequence_length, output_dim]```\nFor example, if:\n\n`batch_size = 32`\n`sequence_length = 200`\n`output_dim = 4`\n\nThe output shape will be:\n\n`[32, 200, 4]`\nEach sample in the sequence is represented by a vector of size output_dim.\n\n\n","metadata":{}},{"cell_type":"code","source":"class ConvolutionalEncoder(nn.Module):\n    def __init__(self, input_channels=1, output_dim=4, repeat_blocks=4):\n        super(ConvolutionalEncoder, self).__init__()\n        \n        # Parallel convolutional paths\n        self.conv1d_128 = nn.Sequential(\n            nn.ReflectionPad1d((63, 64)),\n            nn.Conv1d(input_channels, 100, kernel_size=128, stride=1)\n        )\n        self.conv1d_64 = nn.Sequential(\n            nn.ReflectionPad1d((31, 32)),\n            nn.Conv1d(input_channels, 100, kernel_size=64, stride=1)\n        )\n        self.conv1d_16 = nn.Sequential(\n            nn.ReflectionPad1d((7, 8)),\n            nn.Conv1d(input_channels, 50, kernel_size=16, stride=1)\n        )\n\n        # Dense layer to merge paths\n        self.concat_dense = nn.Linear(100 + 100 + 50, 250)\n\n        # Repeat N=4 blocks\n        self.repeat_blocks = nn.ModuleList([\n            nn.Sequential(\n                nn.ReLU(),\n                nn.BatchNorm1d(250),\n                nn.ReflectionPad1d((31, 32)),\n                nn.Conv1d(250, 250, kernel_size=64, stride=1)\n            ) for _ in range(repeat_blocks)\n        ])\n\n        # Final block\n        self.final_relu = nn.ReLU()\n        self.final_bn = nn.BatchNorm1d(250)\n        self.final_conv = nn.Sequential(\n            nn.ReflectionPad1d((31, 32)),\n            nn.Conv1d(250, output_dim, kernel_size=64, stride=1)\n        )\n        \n    def forward(self, x):\n        # Input shape: [batch_size, sequence_length, channels] modify\n\n        \n        x = x.permute(0, 2, 1)  #[batch_size, channels, sequence_length]\n        \n        # Parallel convolutional paths\n        x1 = self.conv1d_128(x)\n        x2 = self.conv1d_64(x)\n        x3 = self.conv1d_16(x)\n\n        # Concatenate paths\n        x_cat = torch.cat([x1, x2, x3], dim=1)  # [batch_size, 250, sequence_length]\n        \n        # Dense layer\n        x_dense = self.concat_dense(x_cat.permute(0, 2, 1)).permute(0, 2, 1)\n\n        # Repeated blocks\n        x_repeated = x_dense\n        for block in self.repeat_blocks:\n            x_repeated = x_repeated + block(x_repeated)  # Residual connection\n\n        # Final block\n        x_final = self.final_relu(x_repeated)\n        x_final = self.final_bn(x_final)\n        x_final = self.final_conv(x_final)\n        x_final = x_final.permute(0, 2, 1)\n        #print(\"xfinal:\", x_final.shape)\n        return x_final  # [batch_size, output_dim]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:35:07.735596Z","iopub.execute_input":"2024-12-13T11:35:07.736301Z","iopub.status.idle":"2024-12-13T11:35:07.745770Z","shell.execute_reply.started":"2024-12-13T11:35:07.736270Z","shell.execute_reply":"2024-12-13T11:35:07.744895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The **RecurrentEncoder** is a hierarchical recurrent neural network designed to process sequential data, progressively extract features, and combine hierarchical representations. It incorporates GRU layers and a residual recurrent mechanism for deeper temporal learning\n\n___\n\n#### 1. Hierarchical GRU Layers:\n\nThe architecture uses three GRU layers at different levels of abstraction:\n* **GRU (256):** Processes the input sequence at the highest dimensionality.\n* **GRU (128):** Processes downsampled features for mid-level abstraction.\n* **GRU (64):** Processes the most downsampled features for fine-grained details.\n  \nThese layers extract temporal dependencies across different feature representations.\n\n#### 2. Downsampling and Upsampling:\n\n**Downsampling:**\n\nLinear layers reduce the dimensionality between consecutive GRU layers (256 → 128 → 64).\nHelps in compressing features for more efficient learning.\n\n**Upsampling:**\n\nLinear layers increase the dimensionality back (64 → 128 → 256) to unify features at different levels.\n\n#### 3. Feature Concatenation:\n\nFeatures from all GRU layers are concatenated along the feature axis to create a comprehensive representation:\n\n* `[x_256, x_128, x_64]`.\nThis combines information from all levels of the hierarchy.\n\n#### 4. Recurrent Residual Units (RRU):\n\nA series of GRU layers with residual connections are applied to the concatenated features.\nResidual connections help in preserving the original information while learning additional temporal dependencies.\n\nEach RRU consists of:\n\nA layer normalization for stability.\nA GRU to model temporal dynamics.\n5. Output Layer:\n\nA fully connected layer maps the processed hidden representation to the final `output_dim`.\n___\n### Data Flow (forward method)\n**Input:**\nThe input x is a sequence with shape:\n\n`[batch_size, sequence_length, input_dim]`\n\n**Hierarchical Feature Extraction:**\n\n* Data flows through the hierarchical GRU layers:\n  * `x_256` (GRU with 256 units).\n  * Downsampled to `x_128_input`, then passed to GRU (128 units).\n  * Downsampled to `x_64_input`, then passed to GRU (64 units).\n* Each GRU captures temporal dependencies at its respective level.\n\n**Upsampling:**\n\nThe output from the lowest GRU level (`x_64`) is progressively upsampled to match higher-level dimensions.\n\n**Feature Concatenation:**\n\nCombine the outputs from all GRU levels (`x_256`, `x_128`, `x_64`) into a unified representation.\n\n**Residual Recurrent Processing:**\n\nThe concatenated features pass through a series of **residual GRU units** (RRU layers) to refine temporal relationships.\n\n**Output:**\nThe final representation is passed through a dense layer to map it to the desired `output_dim`.\n\nOutput shape:\n\n`[batch_size, sequence_length, output_dim]`\n","metadata":{}},{"cell_type":"code","source":"class RecurrentEncoder(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=128, repeat_n=2):\n        super(RecurrentEncoder, self).__init__()\n\n        self.gru_256 = nn.GRU(input_dim, 256, batch_first=True)\n        self.downsample_256_128 = nn.Linear(256, 128)\n        self.gru_128 = nn.GRU(128, 128, batch_first=True)\n        self.downsample_128_64 = nn.Linear(128, 64)\n        self.gru_64 = nn.GRU(64, 64, batch_first=True)\n\n        self.upsample_64_128 = nn.Linear(64, 128)\n        self.upsample_128_256 = nn.Linear(128, 256)\n\n        self.concat_dense = nn.Linear(256 + 128 + 64, hidden_dim)\n\n        self.rru = nn.ModuleList([\n            nn.Sequential(\n                nn.LayerNorm(hidden_dim),\n                nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n            ) for _ in range(repeat_n)\n        ])\n\n        self.output_dense = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x_256, _ = self.gru_256(x)\n        x_128_input = F.relu(self.downsample_256_128(x_256))\n        x_128, _ = self.gru_128(x_128_input)\n        x_64_input = F.relu(self.downsample_128_64(x_128))\n        x_64, _ = self.gru_64(x_64_input)\n\n        x_128_up = F.relu(self.upsample_64_128(x_64))\n        x_256_up = F.relu(self.upsample_128_256(x_128_up))\n\n        x_concat = torch.cat([x_256, x_128, x_64], dim=-1)\n        x_hidden = F.relu(self.concat_dense(x_concat))\n\n        for i, rru_layer in enumerate(self.rru):\n            residual, _ = rru_layer(x_hidden)\n            x_hidden = x_hidden + residual\n\n        output = self.output_dense(x_hidden)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:35:10.064192Z","iopub.execute_input":"2024-12-13T11:35:10.064570Z","iopub.status.idle":"2024-12-13T11:35:10.073316Z","shell.execute_reply.started":"2024-12-13T11:35:10.064539Z","shell.execute_reply":"2024-12-13T11:35:10.072454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`Projector` and `NTXentLoss`.\n\n### 1. Projector Class\nThe Projector is a neural network designed to process sequential embeddings and reduce their dimensionality while capturing hierarchical representations.\n\nArchitecture:\n\nInput: Sequential data with a specific dimensionality (`input_dim`).\n\nThree Downsampling and BiLSTM Blocks, each block consists of:\n\nA linear layer to reduce dimensionality.\nA BiLSTM (Bidirectional LSTM) to capture temporal dependencies.\nOutputs from the BiLSTM's forward and backward states are concatenated.\nDimensions are progressively reduced from `256 → 128 → 64 → 32`.\nConcatenation:\nThe outputs from all three blocks are concatenated.\nFully Connected Layers:\nTwo dense layers process the concatenated features, reducing them to the final output dimensionality (`output_dim`).\nOutput:\n\nA compact representation of the input with shape `[batch_size, output_dim]`.\n### 2. NTXentLoss Class\nThe NTXentLoss (Normalized Temperature-scaled Cross-Entropy Loss) is a contrastive loss function used for self-supervised learning.\n\nPurpose:\n\nEncourages positive pairs (similar embeddings) to be close in the embedding space.\nPushes negative pairs (dissimilar embeddings) farther apart.\n\nProcess:\n\n* Input Normalization:\nNormalize the embeddings `z_i`, `z_j`, and `z_neg` to unit length.\n\n* Similarity Calculation:\nCompute cosine similarity between:\n`z_i` and `z_j` (positive pairs).\n`z_i` and `z_neg` (negative pairs).\n\n\n* Loss Calculation:\nFor each positive pair:\nNumerator: Exponential similarity of positive pairs.\nDenominator: Sum of exponential similarities for both positive and negative pairs.\nCompute the log-likelihood and take the negative mean as the loss.\nOutput:\n\nA scalar loss value representing how well the embeddings are aligned according to the contrastive objective.","metadata":{}},{"cell_type":"code","source":"class Projector(nn.Module):\n    def __init__(self, input_dim, output_dim=32):\n        super(Projector, self).__init__()\n\n        self.downsample_1 = nn.Linear(input_dim, 256)\n        self.bilstm_256 = nn.LSTM(256, 128, batch_first=True, bidirectional=True)\n\n        self.downsample_2 = nn.Linear(256, 128)\n        self.bilstm_128 = nn.LSTM(128, 64, batch_first=True, bidirectional=True)\n\n        self.downsample_3 = nn.Linear(128, 64)\n        self.bilstm_64 = nn.LSTM(64, 32, batch_first=True, bidirectional=True)\n\n        self.concat_dense_1 = nn.Linear(256 + 128 + 64, 128)\n        self.concat_dense_2 = nn.Linear(128, output_dim)\n\n    def forward(self, x):\n        #print(\"Input x:\", x.shape)\n\n        x_256 = F.relu(self.downsample_1(x))\n        #print(\"Output x_256 (Downsample):\", x_256.shape)\n\n        x_256, (h_256, _) = self.bilstm_256(x_256)\n        flo_256 = torch.cat([h_256[0], h_256[1]], dim=-1)\n        #print(\"Output flo_256 (BiLSTM):\", flo_256.shape)\n\n        x_128 = F.relu(self.downsample_2(x_256))\n        #print(\"Output x_128 (Downsample):\", x_128.shape)\n\n        x_128, (h_128, _) = self.bilstm_128(x_128)\n        flo_128 = torch.cat([h_128[0], h_128[1]], dim=-1)\n        #print(\"Output flo_128 (BiLSTM):\", flo_128.shape)\n\n        x_64 = F.relu(self.downsample_3(x_128))\n        #print(\"Output x_64 (Downsample):\", x_64.shape)\n\n        x_64, (h_64, _) = self.bilstm_64(x_64)\n        flo_64 = torch.cat([h_64[0], h_64[1]], dim=-1)\n        #print(\"Output flo_64 (BiLSTM):\", flo_64.shape)\n\n        x_concat = torch.cat([flo_256, flo_128, flo_64], dim=-1)\n        #print(\"Output x_concat (Concat):\", x_concat.shape)\n\n        x_hidden = F.relu(self.concat_dense_1(x_concat))\n        #print(\"Output x_hidden (Dense):\", x_hidden.shape)\n\n        output = self.concat_dense_2(x_hidden)\n        #print(\"Final Output projector:\", output.shape)\n        return output\n        \nclass NTXentLoss(nn.Module):\n    def __init__(self, temperature=0.05):\n        super(NTXentLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j, z_neg):\n        z_i = F.normalize(z_i, p=2, dim=-1)\n        z_j = F.normalize(z_j, p=2, dim=-1)\n        z_neg = F.normalize(z_neg, p=2, dim=-1)\n\n        # Similarità\n        sim_ij = torch.matmul(z_i, z_j.T) / self.temperature  # Positiva\n        sim_neg = torch.matmul(z_i, z_neg.T) / self.temperature  # Negative\n\n        # Calcolo della loss\n        numerator = torch.exp(sim_ij.diag())\n        denominator = numerator + torch.sum(torch.exp(sim_neg), dim=1)\n\n        loss = -torch.log(numerator / denominator)\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:35:11.803042Z","iopub.execute_input":"2024-12-13T11:35:11.803669Z","iopub.status.idle":"2024-12-13T11:35:11.814879Z","shell.execute_reply.started":"2024-12-13T11:35:11.803629Z","shell.execute_reply":"2024-12-13T11:35:11.813810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. CompleteContrastiveDataset Class\n\nThis dataset class preprocesses the input files and prepares data chunks for contrastive learning.\n\n**Initialization:**\n\nLoads .npy files from the provided paths.\nSplits each channel of the data into chunks of a specified size (`chunk_size=4000`).\n\n**Positive and Negative Samples:**\n* Applies random transformations to the same chunk to create a positive pair.\n* Selects a chunk from a different file to serve as a negative sample.\n\n**Output:**\n\nFor each sample, the dataset returns:\n**Positive Pair:** Two transformed versions of the same chunk (`data1`, `data2`).\n**Negative Sample:** A chunk from a different source (`negative`).\n\n### 3. Training Function (train_contrastive_learning)\n\n**Dataset and Dataloader:**\n\nUses the `CompleteContrastiveDataset` to create batches of positive and negative samples.\n\n**Loss Function:**\n\nUses the NTXentLoss, which encourages positive pairs to have similar embeddings while separating negative pairs.\n\n**Training Steps:**\n\n* Forward pass: Processes the positive pairs and negative samples through the encoder and projector.\n* Loss computation: Calculates the contrastive loss for the batch.\n* Backward pass: Optimizes the encoder and projector to minimize the loss.\n  \n### Workflow\n**Dataset Creation:**\nLoads EEG data from `.npy` files.\n\nSplits each signal into chunks of fixed size (chunk_size=4000).\n\nApplies transformations to create positive and negative pairs.\n\n**Model Training:**\n\nThe encoder and projector process the chunks.\nThe loss function aligns the embeddings of positive pairs and separates negative pairs.\n","metadata":{}},{"cell_type":"code","source":"#------------------ SEED ENCODER ---------------\nimport matplotlib.pyplot as plt\n\n# Funzione per plottare segnali trasformati e negativi\ndef plot_transformed_chunks(data1, data2, negative, idx):\n    plt.figure(figsize=(15, 5))\n\n    # Plotta il primo chunk trasformato\n    plt.subplot(1, 3, 1)\n    plt.plot(data1.squeeze().cpu().numpy(), label='Trasformazione 1', color='blue')\n    plt.title(f'Trasformazione 1 - Chunk {idx}')\n    plt.xlabel('Campioni')\n    plt.ylabel('Ampiezza')\n    plt.legend()\n\n    # Plotta il secondo chunk trasformato\n    plt.subplot(1, 3, 2)\n    plt.plot(data2.squeeze().cpu().numpy(), label='Trasformazione 2', color='green')\n    plt.title(f'Trasformazione 2 - Chunk {idx}')\n    plt.xlabel('Campioni')\n    plt.ylabel('Ampiezza')\n    plt.legend()\n\n    # Plotta il chunk negativo\n    plt.subplot(1, 3, 3)\n    plt.plot(negative.squeeze().cpu().numpy(), label='Chunk Negativo', color='red')\n    plt.title(f'Chunk Negativo - Chunk {idx}')\n    plt.xlabel('Campioni')\n    plt.ylabel('Ampiezza')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\nclass CompleteContrastiveDataset(Dataset):\n    def __init__(self, file_paths, chunk_size=4000):\n        self.all_chunks = []  # Contiene tutti i chunk\n        self.chunk_sources = []  # Contiene il file di origine di ogni chunk\n\n        for file_path in file_paths:\n            # Carica il file NumPy\n            data = np.load(file_path)\n\n            # Dividi ogni canale in chunk\n            for channel_idx in range(data.shape[0]):  # Itera su tutti i canali\n                channel_data = data[channel_idx]\n                chunks = chunk_data(channel_data, chunk_size)\n\n                # Aggiungi i chunk e le loro sorgenti\n                self.all_chunks.extend(chunks)\n                self.chunk_sources.extend([file_path] * len(chunks))\n\n        print(f\"Dataset creato con {len(self.all_chunks)} chunk totali.\")\n\n    def __len__(self):\n        return len(self.all_chunks)\n\n    def __getitem__(self, idx):\n        # Chunk corrispondente\n        chunk = self.all_chunks[idx]\n        source_file = self.chunk_sources[idx]\n\n        # Crea trasformazioni per le coppie positive\n        transformed_1, transformed_2 = apply_random_transformations(chunk)\n\n        # Seleziona un chunk negativo proveniente da un file diverso\n        possible_negatives = [\n            i for i, src in enumerate(self.chunk_sources) if src != source_file\n        ]\n        neg_idx = random.choice(possible_negatives)\n        negative_chunk = self.all_chunks[neg_idx]\n\n        data1 = torch.tensor(transformed_1, dtype=torch.float32).unsqueeze(-1)\n        data2 = torch.tensor(transformed_2, dtype=torch.float32).unsqueeze(-1)\n        negative = torch.tensor(negative_chunk, dtype=torch.float32).unsqueeze(-1)\n\n        return data1, data2, negative\n\n\n\n# ------------------- Funzione di Addestramento -------------------\ndef train_contrastive_learning(model_type, file_paths, encoder, projector, optimizer, epochs=10, batch_size=10, device=\"cuda\"):\n    loss_fn = NTXentLoss()\n\n    \n    dataset = CompleteContrastiveDataset(file_paths, chunk_size=4000)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    \n    # Parallelizzazione su più GPU\n    encoder = nn.DataParallel(encoder)\n    projector = nn.DataParallel(projector)\n    encoder.to(device)\n    projector.to(device)\n\n    for epoch in range(epochs):\n        print(f\"=== Epoca {epoch + 1}/{epochs} ===\")\n        total_loss = 0\n\n        for batch_idx, (data1, data2, neg_data) in enumerate(dataloader):\n            data1, data2, neg_data= (\n                data1.to(device),\n                data2.to(device),\n                neg_data.to(device),\n            )\n\n            # Passaggio attraverso encoder e projector\n            z1 = projector(encoder(data1))\n            z2 = projector(encoder(data2))\n            z_neg = projector(encoder(neg_data))\n\n            # Calcolo della loss\n            loss = loss_fn(z1, z2, z_neg)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            print(f\"Batch {batch_idx + 1}/{len(dataloader)}, Perdita: {loss.item():.4f}\")\n            #plot_transformed_chunks(data1[batch_idx], data2[batch_idx], neg_data[batch_idx], idx=batch_idx)\n\n        print(f\"Perdita totale per epoca {epoch + 1}: {total_loss:.4f}\")\n        save_model(encoder, os.path.join(save_path, f\"{model_type}_epoch_{epoch + 1}.pth\"))\n        save_model(projector, os.path.join(save_path, f\"projector_{model_type}_epoch_{epoch + 1}.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:35:13.752763Z","iopub.execute_input":"2024-12-13T11:35:13.753109Z","iopub.status.idle":"2024-12-13T11:35:13.768627Z","shell.execute_reply.started":"2024-12-13T11:35:13.753078Z","shell.execute_reply":"2024-12-13T11:35:13.767665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Esecuzione recurrent encoder training\n\nif __name__ == \"__main__\":\n    folder_path = \"/kaggle/working/preprocessed\"\n    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.npy')]\n\n    encoder = RecurrentEncoder(input_dim=1, output_dim=4)\n    projector = Projector(input_dim=4, output_dim=32)\n\n    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(projector.parameters()), lr=1e-4)\n    train_contrastive_learning( \"Recurrent\", file_paths, encoder, projector, optimizer, epochs=30, batch_size= 40, device=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:08:46.276278Z","iopub.execute_input":"2024-12-13T11:08:46.276647Z","iopub.status.idle":"2024-12-13T11:09:01.094831Z","shell.execute_reply.started":"2024-12-13T11:08:46.276616Z","shell.execute_reply":"2024-12-13T11:09:01.093410Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Esecuzione covnolutional encoder training\nif __name__ == \"__main__\":\n    folder_path = \"/kaggle/working/preprocessed\" \n    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.npy')]\n\n    encoder = ConvolutionalEncoder(input_channels=1, output_dim=4)\n    projector = Projector(input_dim=4, output_dim=32)\n\n    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(projector.parameters()), lr=1e-4)\n\n    train_contrastive_learning(\"Convolutional\", file_paths, encoder, projector, optimizer, epochs=30, batch_size=40, device=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:13:36.675240Z","iopub.execute_input":"2024-12-12T17:13:36.675865Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}